import json
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling
)
from peft import get_peft_model, LoraConfig, TaskType

# âœ… ëª¨ë¸ëª… ë° ê²½ë¡œ
model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
output_dir = "/home/alpaco/lcmtest/naver/finetuned_hyperclovax30"

# âœ… í† í¬ë‚˜ì´ì € / ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

# âœ… PEFT (LoRA) ì„¤ì •
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none"
)
model = get_peft_model(model, peft_config)

# âœ… ë°ì´í„° ë¡œë“œ
with open("/home/alpaco/welhome/PEFT/fine_data.json", encoding="utf-8") as f:
    raw_data = json.load(f)

dataset = Dataset.from_list([
    {"instruction": item["instruction"], "output": item["output"]}
    for item in raw_data
])

# âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess(example):
    prompt_text = tokenizer.apply_chat_template(
        [
            {"role": "tool_list", "content": ""},
            {"role": "system", "content": "- AI ì–¸ì–´ëª¨ë¸ì˜ ì´ë¦„ì€ \"CLOVA X\" ì´ë©° ë„¤ì´ë²„ì—ì„œ ë§Œë“¤ì—ˆë‹¤.\n- ì˜¤ëŠ˜ì€ 2025ë…„ 04ì›” 24ì¼(ëª©)ì´ë‹¤."},
            {"role": "user", "content": example["instruction"]}
        ],
        add_generation_prompt=True,
        tokenize=False
    )
    full_text = prompt_text + example["output"]

    tokenized = tokenizer(
        full_text,
        return_tensors="pt",
        max_length=1024,
        truncation=True,
        padding="max_length"
    )

    input_ids = tokenized["input_ids"][0]
    attention_mask = tokenized["attention_mask"][0]
    labels = input_ids.clone()

    prompt_len = len(tokenizer(prompt_text, return_tensors="pt")["input_ids"][0])
    labels[:prompt_len] = -100

    return {
        "input_ids": input_ids.tolist(),
        "attention_mask": attention_mask.tolist(),
        "labels": labels.tolist()
    }

# âœ… ë°ì´í„° ë¶„í• 
dataset_split = dataset.train_test_split(test_size=0.2, seed=42)
train_data = dataset_split["train"].map(preprocess)
val_data = dataset_split["test"].map(preprocess)

# âœ… ë””ë²„ê¹…ìš© ìƒ˜í”Œ ì¶œë ¥
print("\nğŸ§ª [DEBUG SAMPLE 2ê°œ]")
for i in range(2):
    sample = train_data[i]
    decoded_input = tokenizer.decode(sample["input_ids"], skip_special_tokens=False)
    decoded_label = tokenizer.decode(
        [id if id != -100 else tokenizer.pad_token_id for id in sample["labels"]],
        skip_special_tokens=False
    )
    prompt_len = sample["labels"].index(
        next(label for label in sample["labels"] if label != -100)
    )

    print(f"\nğŸ”¹ [Sample {i+1}]")
    print(f"ğŸ”¸ instruction : {dataset_split['train'][i]['instruction']}")
    print(f"ğŸ”¸ output      : {dataset_split['train'][i]['output']}")
    print(f"ğŸ”¸ prompt_len  : {prompt_len}")
    print(f"ğŸ”¸ input_ids[:30]: {sample['input_ids'][:30]}")
    print(f"ğŸ”¸ labels[:30]   : {sample['labels'][:30]}")
    print(f"ğŸ”¸ decoded_input:\n{decoded_input}")
    print(f"ğŸ”¸ decoded_label:\n{decoded_label}")

# âœ… ë°ì´í„° ì½œë ˆì´í„°
collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# âœ… í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=30,
    learning_rate=5e-5,
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch",
    bf16=True,
    report_to="none"
)

# âœ… Trainer êµ¬ì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
    tokenizer=tokenizer,
    data_collator=collator
)

# âœ… í•™ìŠµ ì‹œì‘
print("\nğŸš€ í•™ìŠµ ì‹œì‘")
trainer.train()

# âœ… ìµœì¢… í‰ê°€
print("\nğŸ§ª ìµœì¢… ê²€ì¦:")
metrics = trainer.evaluate()
print(metrics)

# âœ… ëª¨ë¸ ì €ì¥
print("\nâœ… í•™ìŠµ ì™„ë£Œ - ëª¨ë¸ ì €ì¥ ì¤‘...")
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
model.config.save_pretrained(output_dir)
print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_dir}")
