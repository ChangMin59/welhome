import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

#  ë³‘í•©ëœ ëª¨ë¸ ê²½ë¡œ
merged_model_path = "/home/alpaco/test/fine/merged_model"
base_tokenizer_path = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"

#  ë³‘í•©ëœ ëª¨ë¸ ë¡œë“œ (PeftModel âŒ)
model = AutoModelForCausalLM.from_pretrained(
    merged_model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
).eval()

# âœ… í† í¬ë‚˜ì´ì €ëŠ” ì›ë³¸ ëª¨ë¸ì˜ ê²ƒì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_path)

# âœ… ì¶”ë¡  í•¨ìˆ˜
def ask_clovax_clean(question: str, max_new_tokens=128) -> str:
    messages = [
        {"role": "tool_list", "content": ""},
        {"role": "system", "content": "- AI ì–¸ì–´ëª¨ë¸ì˜ ì´ë¦„ì€ \"CLOVA X\" ì´ë©° ë„¤ì´ë²„ì—ì„œ ë§Œë“¤ì—ˆë‹¤.\n- ì˜¤ëŠ˜ì€ 2025ë…„ 04ì›” 24ì¼(ëª©)ì´ë‹¤."},
        {"role": "user", "content": question}
    ]

    enc = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
    input_ids = enc.to(model.device) if isinstance(enc, torch.Tensor) else enc["input_ids"].to(model.device)
    attention_mask = enc["attention_mask"].to(model.device) if isinstance(enc, dict) else None

    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=False,
            repetition_penalty=1.1,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )

    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=False)

    # ğŸ§¹ ì‘ë‹µ íŒŒì‹±
    if "<|im_start|>assistant" in decoded:
        response = decoded.split("<|im_start|>assistant")[-1]
        return response.replace("<|im_end|>", "").split("<|")[0].strip()

    return decoded.strip()