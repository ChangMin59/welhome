import torch
from transformers import AutoModelForCausalLM
from peft import PeftModel

# ê²½ë¡œ ì„¤ì •
base_model_path = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"  # ì›ë³¸ ëª¨ë¸
adapter_path = "/home/alpaco/test/fine/finetuned_hyperclovax30"            # LoRA ì–´ëŒ‘í„° ê²½ë¡œ
merged_output_path = "/home/alpaco/test/fine/merged_model"                 # ë³‘í•© ê²°ê³¼ ì €ì¥ ê²½ë¡œ

# 1. ì›ë³¸ ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# 2. LoRA ì–´ëŒ‘í„° ë¡œë“œ ë° ë³‘í•©
print("ğŸš€ LoRA ì–´ëŒ‘í„° ë³‘í•© ì¤‘...")
lora_model = PeftModel.from_pretrained(base_model, adapter_path)
merged_model = lora_model.merge_and_unload()

# 3. ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
merged_model.save_pretrained(merged_output_path)
print(f"âœ… ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {merged_output_path}")
